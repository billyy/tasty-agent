# Required TastyTrade credentials
TASTYTRADE_CLIENT_SECRET=your_secret
TASTYTRADE_REFRESH_TOKEN=your_token
TASTYTRADE_ACCOUNT_ID=your_account_id

# Logging (optional)
# LOG_LEVEL=WARNING  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

# Agent Custom Rules (optional)
# These rules apply to EVERY interaction with the agent
# AGENT_CUSTOM_RULES=Never trade on Fridays. Max position size is 5% of portfolio. Focus on high IV rank symbols.

# LLM Configuration
# Default: openai:gpt-4o-mini
#
# OpenAI Models:
#   - openai:gpt-4o-mini (fast, cheap - $0.15/1M input tokens)
#   - openai:gpt-4o (balanced - $2.50/1M input tokens)
#   - openai:o1-mini (reasoning model)
#   - openai:gpt-3.5-turbo (legacy, cheap)
#
# Other Providers:
#   - anthropic:claude-3-5-haiku-20241022 (fastest Claude)
#   - groq:llama-3.1-70b-versatile (very fast, free tier)
#   - gemini:gemini-1.5-flash (fast, generous free tier)
MODEL_IDENTIFIER=openai:gpt-4o-mini

# Required API keys (depending on MODEL_IDENTIFIER)
OPENAI_API_KEY=your_key
ANTHROPIC_API_KEY=your_key  # If using Claude
GROQ_API_KEY=your_key  # If using Groq
GOOGLE_API_KEY=your_key  # If using Gemini

# ===================================================================
# OpenAI-Compatible APIs (Local/Self-hosted)
# ===================================================================

# Uncomment ONE of these configurations:

# --- Ollama (easiest local setup) ---
# 1. Install: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull model: ollama pull llama3.1:8b
# 3. Start server: ollama serve
#MODEL_IDENTIFIER=openai:llama3.1:8b
#OPENAI_BASE_URL=http://localhost:11434/v1
#OPENAI_API_KEY=ollama  # Any value works with Ollama

# --- vLLM (fast inference) ---
# 1. Install: pip install vllm
# 2. Start: vllm serve meta-llama/Llama-3.1-8B-Instruct --port 8000
#MODEL_IDENTIFIER=openai:meta-llama/Llama-3.1-8B-Instruct
#OPENAI_BASE_URL=http://localhost:8000/v1
#OPENAI_API_KEY=dummy  # Any value works

# --- LM Studio (GUI for local models) ---
# 1. Download from lmstudio.ai
# 2. Load a model and start local server on port 1234
#MODEL_IDENTIFIER=openai:local-model
#OPENAI_BASE_URL=http://localhost:1234/v1
#OPENAI_API_KEY=lm-studio

# --- llama.cpp (OpenAI server mode) ---
# 1. Build: https://github.com/ggerganov/llama.cpp
# 2. Start: ./llama-server -m model.gguf --port 8080
#MODEL_IDENTIFIER=openai:llama-model
#OPENAI_BASE_URL=http://localhost:8080/v1
#OPENAI_API_KEY=dummy
